{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook will be for writing codes for basic ML like loss functions, optimization functions, etc.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required:\n",
    "1. Weight matrix\n",
    "2. Sample input data\n",
    "3. bias vector\n",
    "\n",
    "Methods:\n",
    "1. Loss calculate:\n",
    "\n",
    "    a. Non - vectorized\n",
    "    b. Half - Vectorized\n",
    "    c. Full - Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Debug(desc, value):\n",
    "    print \"\\n\" + desc +  \"\\n\"\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L2_regularize(W):\n",
    "    W_squared = np.square(W)\n",
    "    L2_reg = np.sum(W_squared)\n",
    "    return L2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: \n",
    "# @y : 1 x N (ground truth labels)\n",
    "# @wts : C x D (class x feature dim)\n",
    "# @datapts : => D x N (features x data points)\n",
    "# @bias : C x 1\n",
    "\n",
    "# output:\n",
    "# loss : SVM loss\n",
    "\n",
    "# TODO: \n",
    "# 1. bias can be incorporated inside wts itself by creating one more column vector and corresponding change in datapts.\n",
    "\n",
    "def svm_loss(wts, datapts, y, bias):\n",
    "    # TODO: check dim of matrices\n",
    "    scores = np.add((np.dot(wts, datapts)), np.repeat(bias[:, np.newaxis], y.size, 1));  # bias will be broadcasted\n",
    "    Debug(\"Scores: \", scores)\n",
    "    \n",
    "    loss_vec = np.zeros(y.size)\n",
    "    delta = 1.0\n",
    "    \n",
    "    ## No vectorization ##\n",
    "    '''\n",
    "    # loop for each data point\n",
    "    for i in range(y.size):\n",
    "        loss_i = 0;\n",
    "        correct_label = y[i]\n",
    "        correct_class_score = scores[correct_label, i]\n",
    "\n",
    "        # loop for number of classes\n",
    "        for j in range(scores.shape[0]):\n",
    "            if(correct_label != j):\n",
    "                hinge_loss_i_j = max(scores[j, i] - correct_class_score + delta, 0)\n",
    "                loss_i += hinge_loss_i_j       \n",
    "        \n",
    "        loss_vec[i] = loss_i\n",
    "    '''\n",
    "    \n",
    "    ## Half - vectorized  ##\n",
    "    '''\n",
    "    for i in range(y.size):\n",
    "        loss_i = 0;\n",
    "        correct_class_score = scores[y[i], i]\n",
    "                \n",
    "        loss_margins = np.maximum(0, scores[:, i] - correct_class_score + delta)\n",
    "        loss_margins[y[i]] = 0   # for correct class margin is 0\n",
    "        \n",
    "        loss_i = np.sum(loss_margins)        \n",
    "        \n",
    "        loss_vec[i] = loss_i\n",
    "    '''\n",
    "    \n",
    "    ##  FULL VECTORIZED ##\n",
    "    ''\n",
    "    #scores_mat = np.repeat(scores[:, np.newaxis], scores.size, axis=1)\n",
    "    #print \"\\nScores Repeated: \\n\"\n",
    "    #print scores_mat\n",
    "    \n",
    "    correct_score_mat = scores[y, np.arange(scores.shape[1])]   # 1 x N\n",
    "    Debug(\"Correct Label Scores:\", correct_score_mat)\n",
    "    \n",
    "    loss_margin = np.maximum(scores - correct_score_mat + delta, 0)\n",
    "    loss_margin[y, np.arange(loss_margin.shape[1])] = 0     # Making loss of actual labels 0\n",
    "    Debug(\"Loss Margin: \", loss_margin)\n",
    "    \n",
    "    loss_vec = np.sum(loss_margin, axis=0)\n",
    "    ''\n",
    "    \n",
    "    # Regularize\n",
    "    reg_rate = 0.5\n",
    "    loss_vec_reg = np.add(loss_vec, reg_rate * L2_regularize(wts))\n",
    "    \n",
    "    full_loss = np.sum(loss_vec_reg) / loss_vec_reg.size\n",
    "    \n",
    "    return full_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_data():\n",
    "    #wts = np.random.random((3,4))\n",
    "    wts = np.array([ [  1,   5,   9,   9],\n",
    "                     [  6,   3,   3,   6],\n",
    "                     [  7,   5,   8,   9]\n",
    "                    ], dtype='f')\n",
    "    \n",
    "    # TODO: Center the data\n",
    "    datapts = np.array([[2, 1, 4, 3],\n",
    "                       [5, 8, 2, 9],\n",
    "                       [11, 4, 1, 6],\n",
    "                       [7, 3, 6, 5]\n",
    "                       ], dtype='f')\n",
    "    \n",
    "    y = np.array([1, 0, 2, 1])\n",
    "    \n",
    "    bias = np.random.random(wts.shape[0])\n",
    "    \n",
    "    return wts, datapts, bias, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svm_loss():\n",
    "    wts, datapts, bias, y = get_sample_data()\n",
    "    Debug(\"Weights:\", wts)\n",
    "    \n",
    "    Debug(\"Data:\", datapts)\n",
    "    \n",
    "    loss = svm_loss(wts, datapts, y, bias)\n",
    "    Debug(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "\n",
      "[[ 1.  5.  9.  9.]\n",
      " [ 6.  3.  3.  6.]\n",
      " [ 7.  5.  8.  9.]]\n",
      "\n",
      "Data:\n",
      "\n",
      "[[  2.   1.   4.   3.]\n",
      " [  5.   8.   2.   9.]\n",
      " [ 11.   4.   1.   6.]\n",
      " [  7.   3.   6.   5.]]\n",
      "\n",
      "Scores: \n",
      "\n",
      "[[ 189.5560484   104.5560484    77.5560484   147.5560484 ]\n",
      " [ 102.96857559   60.96857559   69.96857559   93.96857559]\n",
      " [ 190.36337796  106.36337796  100.36337796  159.36337796]]\n",
      "\n",
      "Correct Label Scores:\n",
      "\n",
      "[ 102.96857559  104.5560484   100.36337796   93.96857559]\n",
      "\n",
      "Loss Margin: \n",
      "\n",
      "[[ 87.58747282   0.           0.          54.58747282]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 88.39480238   2.80732956   0.          66.39480238]]\n",
      "\n",
      "Loss:\n",
      "\n",
      "323.442969988\n"
     ]
    }
   ],
   "source": [
    "test_svm_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Take care of numerical stability for high scores\n",
    "# 2. Regularization\n",
    "\n",
    "def softmax_loss(wts, datapts, y, bias):\n",
    "    # TODO: check dim of matrices\n",
    "    scores = np.add((np.dot(wts, datapts)), np.repeat(bias[:, np.newaxis], y.size, 1));  # bias will be broadcasted\n",
    "    Debug(\"Scores: \", scores)\n",
    "    \n",
    "    correct_scores = scores[y, np.arange(scores.shape[1])]\n",
    "    Debug(\"Correct Scores:\", correct_scores)\n",
    "\n",
    "    correct_scores_exp = np.exp(correct_scores)\n",
    "    \n",
    "    # TODO: it is better to center the scores for each data point before applying exp and taking sum. \n",
    "    # This will avoid numerical bloating\n",
    "    scores_exp = np.exp(scores)    \n",
    "    scores_exp_sum = np.sum(scores_exp, axis=0)\n",
    "    Debug(\"Exponential Scores Sum:\", scores_exp_sum)\n",
    "    \n",
    "    prob = correct_scores_exp / scores_exp_sum\n",
    "    Debug(\"Softmax output: \", prob)\n",
    "    \n",
    "    log_prob = np.log(prob)\n",
    "    Debug(\"Log prob: \", log_prob)\n",
    "    \n",
    "    loss = -1 * np.sum(log_prob) / y.size\n",
    "    \n",
    "    # Regularize\n",
    "    reg_rate = 0.5\n",
    "    loss_reg = loss + reg_rate * L2_regularize(wts)\n",
    "    \n",
    "    return loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_loss():\n",
    "    wts, datapts, bias, y = get_sample_data()\n",
    "    Debug(\"Weights:\", wts)\n",
    "    \n",
    "    Debug(\"Data:\", datapts)\n",
    "    \n",
    "    loss = softmax_loss(wts, datapts, y, bias)\n",
    "    Debug(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "\n",
      "[[ 1.  5.  9.  9.]\n",
      " [ 6.  3.  3.  6.]\n",
      " [ 7.  5.  8.  9.]]\n",
      "\n",
      "Data:\n",
      "\n",
      "[[  2.   1.   4.   3.]\n",
      " [  5.   8.   2.   9.]\n",
      " [ 11.   4.   1.   6.]\n",
      " [  7.   3.   6.   5.]]\n",
      "\n",
      "Scores: \n",
      "\n",
      "[[ 189.13368041  104.13368041   77.13368041  147.13368041]\n",
      " [ 102.24502296   60.24502296   69.24502296   93.24502296]\n",
      " [ 190.67718639  106.67718639  100.67718639  159.67718639]]\n",
      "\n",
      "Correct Scores:\n",
      "\n",
      "[ 102.24502296  104.13368041  100.67718639   93.24502296]\n",
      "\n",
      "Exponential Scores Sum:\n",
      "\n",
      "[  7.83675849e+82   2.30234253e+46   5.29110648e+43   2.22291309e+69]\n",
      "\n",
      "Softmax output: \n",
      "\n",
      "[  3.23826817e-39   7.28639729e-02   1.00000000e+00   1.40888795e-29]\n",
      "\n",
      "Log prob: \n",
      "\n",
      "[ -8.86257800e+01  -2.61916096e+00  -5.96139804e-11  -6.64321670e+01]\n",
      "\n",
      "Loss:\n",
      "\n",
      "287.919276977\n"
     ]
    }
   ],
   "source": [
    "test_softmax_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:theano27]",
   "language": "python",
   "name": "conda-env-theano27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
