{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook will be for writing codes for basic ML like loss functions, optimization functions, etc.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required:\n",
    "1. Weight matrix\n",
    "2. Sample input data\n",
    "3. bias vector\n",
    "\n",
    "Methods:\n",
    "1. Loss calculate:\n",
    "\n",
    "    a. Non - vectorized\n",
    "    b. Half - Vectorized\n",
    "    c. Full - Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Debug(desc, value):\n",
    "    print \"\\n\" + desc +  \"\\n\"\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: \n",
    "# @y : 1 x N (ground truth labels)\n",
    "# @wts : C x D (class x feature dim)\n",
    "# @datapts : => D x N (features x data points)\n",
    "# @bias : C x 1\n",
    "\n",
    "# output:\n",
    "# loss : SVM loss\n",
    "\n",
    "# TODO: \n",
    "# 1. bias can be incorporated inside wts itself by creating one more column vector and corresponding change in datapts.\n",
    "\n",
    "def svm_loss(wts, datapts, y, bias):\n",
    "    # TODO: check dim of matrices\n",
    "    scores = np.add((np.dot(wts, datapts)), np.repeat(bias[:, np.newaxis], y.size, 1));  # bias will be broadcasted\n",
    "    Debug(\"Scores: \", scores)\n",
    "    \n",
    "    loss_vec = np.zeros(y.size)\n",
    "    delta = 1.0\n",
    "    \n",
    "    ## No vectorization ##\n",
    "    '''\n",
    "    # loop for each data point\n",
    "    for i in range(y.size):\n",
    "        loss_i = 0;\n",
    "        correct_label = y[i]\n",
    "        correct_class_score = scores[correct_label, i]\n",
    "\n",
    "        # loop for number of classes\n",
    "        for j in range(scores.shape[0]):\n",
    "            if(correct_label != j):\n",
    "                hinge_loss_i_j = max(scores[j, i] - correct_class_score + delta, 0)\n",
    "                loss_i += hinge_loss_i_j       \n",
    "        \n",
    "        loss_vec[i] = loss_i\n",
    "    '''\n",
    "    \n",
    "    ## Half - vectorized  ##\n",
    "    '''\n",
    "    for i in range(y.size):\n",
    "        loss_i = 0;\n",
    "        correct_class_score = scores[y[i], i]\n",
    "                \n",
    "        loss_margins = np.maximum(0, scores[:, i] - correct_class_score + delta)\n",
    "        loss_margins[y[i]] = 0   # for correct class margin is 0\n",
    "        print \"\\nLoss Margin: \\n\"\n",
    "        print loss_margins\n",
    "        \n",
    "        loss_i = np.sum(loss_margins)        \n",
    "        \n",
    "        loss_vec[i] = loss_i\n",
    "    '''\n",
    "    \n",
    "    ##  FULL VECTORIZED ##\n",
    "    ''\n",
    "    #scores_mat = np.repeat(scores[:, np.newaxis], scores.size, axis=1)\n",
    "    #print \"\\nScores Repeated: \\n\"\n",
    "    #print scores_mat\n",
    "    \n",
    "    correct_score_mat = scores[y, np.arange(scores.shape[1])]   # 1 x N\n",
    "    Debug(\"Correct Label Scores:\", correct_score_mat)\n",
    "    \n",
    "    loss_margin = np.maximum(scores - correct_score_mat + delta, 0)\n",
    "    loss_margin[y, np.arange(loss_margin.shape[1])] = 0     # Making loss of actual labels 0\n",
    "    Debug(\"Loss Margin: \", loss_margin)\n",
    "    \n",
    "    loss_vec = np.sum(loss_margin, axis=0)\n",
    "    ''\n",
    "    \n",
    "    full_loss = np.sum(loss_vec) / loss_vec.size\n",
    "    \n",
    "    return full_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_data():\n",
    "    #wts = np.random.random((3,4))\n",
    "    wts = np.array([ [  1,   5,   9,   9],\n",
    "                     [  6,   3,   3,   6],\n",
    "                     [  7,   5,   8,   9]\n",
    "                    ], dtype='f')\n",
    "    \n",
    "    # TODO: Center the data\n",
    "    datapts = np.array([[2, 1, 4, 3],\n",
    "                       [5, 8, 2, 9],\n",
    "                       [11, 4, 1, 6],\n",
    "                       [7, 3, 6, 5]\n",
    "                       ], dtype='f')\n",
    "    \n",
    "    y = np.array([1, 0, 2, 1])\n",
    "    \n",
    "    bias = np.random.random(wts.shape[0])\n",
    "    \n",
    "    return wts, datapts, bias, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svm_loss():\n",
    "    wts, datapts, bias, y = get_sample_data()\n",
    "    Debug(\"Weights:\", wts)\n",
    "    \n",
    "    Debug(\"Data:\", datapts)\n",
    "    \n",
    "    loss = svm_loss(wts, datapts, y, bias)\n",
    "    Debug(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "\n",
      "[[ 1.  5.  9.  9.]\n",
      " [ 6.  3.  3.  6.]\n",
      " [ 7.  5.  8.  9.]]\n",
      "\n",
      "Data:\n",
      "\n",
      "[[  2.   1.   4.   3.]\n",
      " [  5.   8.   2.   9.]\n",
      " [ 11.   4.   1.   6.]\n",
      " [  7.   3.   6.   5.]]\n",
      "\n",
      "Scores: \n",
      "\n",
      "[[ 189.80691509  104.80691509   77.80691509  147.80691509]\n",
      " [ 102.56272565   60.56272565   69.56272565   93.56272565]\n",
      " [ 190.05670762  106.05670762  100.05670762  159.05670762]]\n",
      "\n",
      "Correct Label Scores:\n",
      "\n",
      "[ 102.56272565  104.80691509  100.05670762   93.56272565]\n",
      "\n",
      "Loss Margin: \n",
      "\n",
      "[[ 88.24418944   0.           0.          55.24418944]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 88.49398198   2.24979254   0.          66.49398198]]\n",
      "\n",
      "Loss:\n",
      "\n",
      "75.1815338413\n"
     ]
    }
   ],
   "source": [
    "test_svm_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Take care of numerical stability for high scores\n",
    "# 2. Regularization\n",
    "\n",
    "def softmax_loss(wts, datapts, y, bias):\n",
    "    # TODO: check dim of matrices\n",
    "    scores = np.add((np.dot(wts, datapts)), np.repeat(bias[:, np.newaxis], y.size, 1));  # bias will be broadcasted\n",
    "    Debug(\"Scores: \", scores)\n",
    "    \n",
    "    correct_scores = scores[y, np.arange(scores.shape[1])]\n",
    "    Debug(\"Correct Scores:\", correct_scores)\n",
    "\n",
    "    correct_scores_exp = np.exp(correct_scores)\n",
    "    \n",
    "    # TODO: it is better to center the scores for each data point before applying exp and taking sum. \n",
    "    # This will avoid numerical bloating\n",
    "    scores_exp = np.exp(scores)    \n",
    "    scores_exp_sum = np.sum(scores_exp, axis=0)\n",
    "    Debug(\"Exponential Scores Sum:\", scores_exp_sum)\n",
    "    \n",
    "    prob = correct_scores_exp / scores_exp_sum\n",
    "    Debug(\"Softmax output: \", prob)\n",
    "    \n",
    "    log_prob = np.log(prob)\n",
    "    Debug(\"Log prob: \", log_prob)\n",
    "    \n",
    "    loss = -1 * np.sum(log_prob) / y.size\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_loss():\n",
    "    wts, datapts, bias, y = get_sample_data()\n",
    "    Debug(\"Weights:\", wts)\n",
    "    \n",
    "    Debug(\"Data:\", datapts)\n",
    "    \n",
    "    loss = softmax_loss(wts, datapts, y, bias)\n",
    "    Debug(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:\n",
      "\n",
      "[[ 1.  5.  9.  9.]\n",
      " [ 6.  3.  3.  6.]\n",
      " [ 7.  5.  8.  9.]]\n",
      "\n",
      "Data:\n",
      "\n",
      "[[  2.   1.   4.   3.]\n",
      " [  5.   8.   2.   9.]\n",
      " [ 11.   4.   1.   6.]\n",
      " [  7.   3.   6.   5.]]\n",
      "\n",
      "Scores: \n",
      "\n",
      "[[ 189.39185846  104.39185846   77.39185846  147.39185846]\n",
      " [ 102.19897635   60.19897635   69.19897635   93.19897635]\n",
      " [ 190.56834248  106.56834248  100.56834248  159.56834248]]\n",
      "\n",
      "Correct Scores:\n",
      "\n",
      "[ 102.19897635  104.39185846  100.56834248   93.19897635]\n",
      "\n",
      "Exponential Scores Sum:\n",
      "\n",
      "[  7.57717233e+82   2.13161998e+46   4.74543681e+43   1.99366812e+69]\n",
      "\n",
      "Softmax output: \n",
      "\n",
      "[  3.19848500e-39   1.01882198e-01   1.00000000e+00   1.50019696e-29]\n",
      "\n",
      "Log prob: \n",
      "\n",
      "[ -8.86381414e+01  -2.28393806e+00  -8.60401750e-11  -6.63693713e+01]\n",
      "\n",
      "Loss:\n",
      "\n",
      "39.3228626776\n"
     ]
    }
   ],
   "source": [
    "test_softmax_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:theano27]",
   "language": "python",
   "name": "conda-env-theano27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
